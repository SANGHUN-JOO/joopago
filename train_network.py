# -*- coding: utf-8 -*-
"""train_network.py

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/136Upn3QBhVHLVTC6vc0-FiR2s4cM7DHH
"""

from dual_network import DN_INPUT_SHAPE
from tensorflow.keras.callbacks import LearningRateScheduler, LambdaCallback
from tensorflow.keras.models import load_model
from tensorflow.keras import backend as K
from pathlib import Path
import numpy as np
import pickle

RN_EPOCHS = 1000 #original 500,000

def load_data():
    cur_dir = Path(__file__).parent.absolute()
    cur_dir = cur_dir / 'data'
    history_path = sorted(cur_dir.glob('*.history'))[-1]
    with history_path.open(mode='rb') as f:
        return pickle.load(f)

def train_network(p_fail_count):
    history = load_data()
    xs, y_policies, y_values = zip(*history)

    xs = np.array(xs)

    y_policies = np.array(y_policies)
    y_values = np.array(y_values)

    cur_dir = Path(__file__).parent.absolute()
    model = load_model(str(cur_dir) + '\\model\\best.h5')

    model.compile(loss=['categorical_crossentropy', 'mse'], optimizer='adam')

    def step_decay(epoch):
        x = 0.08   # orignal 0.02
        if epoch >= 200: x = 0.002   # epoch >= 300 : 0.002
        if epoch >= 800: x = 0.0002 # epoch >= 500000 : 0.0002
        return x
    
    lr_decay = LearningRateScheduler(step_decay)

    print_callback = LambdaCallback(
        on_epoch_begin=lambda epoch, logs:
            print('\rTrain {}/{}'.format(epoch+1, RN_EPOCHS), end='')
    )

    epoch_count = RN_EPOCHS + p_fail_count*40

    model.fit(xs, [y_policies, y_values], batch_size=32, epochs=epoch_count, # orignal_batch_size = 2048
              verbose=0, callbacks=[lr_decay, print_callback])
    print('')


    model.save(str(cur_dir) + '\\model\\latest.h5')

    K.clear_session()
    del model

if __name__ == '__main__':
    train_network()